import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
import librosa
 
# Simulate loading MFCC features from audio clips
def generate_mfcc(label, base_freq, n_samples=100):
    data = []
    for _ in range(n_samples):
        # Generate synthetic tone (or use real audio loading)
        tone = np.sin(2 * np.pi * np.linspace(0, 1, 16000) * base_freq)
        mfcc = librosa.feature.mfcc(y=tone, sr=16000, n_mfcc=13)
        data.append(mfcc.T[:100])  # fixed size: 100 time steps
    labels = [label] * n_samples
    return np.array(data), labels
 
# Generate synthetic MFCC sets for 3 sound classes
clap_X, clap_y = generate_mfcc(0, 500)
glass_X, glass_y = generate_mfcc(1, 1500)
noise_X, noise_y = generate_mfcc(2, 250)
 
# Combine and encode
X = np.concatenate([clap_X, glass_X, noise_X])
y = np.concatenate([clap_y, glass_y, noise_y])
 
# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
 
# Build 1D CNN model for MFCC input
model = models.Sequential([
    layers.Input(shape=(100, 13)),  # 100 time steps, 13 MFCCs
    layers.Conv1D(32, kernel_size=3, activation='relu'),
    layers.MaxPooling1D(pool_size=2),
    layers.Conv1D(64, kernel_size=3, activation='relu'),
    layers.GlobalMaxPooling1D(),
    layers.Dense(64, activation='relu'),
    layers.Dense(3, activation='softmax')  # 3 audio classes
])
 
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
 
# Evaluate
loss, acc = model.evaluate(X_test, y_test)
print(f"âœ… Edge Audio Classifier Accuracy: {acc:.4f}")
 
# Predict 5 test samples
preds = np.argmax(model.predict(X_test[:5]), axis=1)
label_map = {0: "Clap", 1: "Glass Break", 2: "Noise"}
for i, pred in enumerate(preds):
    print(f"Sample {i+1}: Predicted = {label_map[pred]}, Actual = {label_map[y_test[i]]}")